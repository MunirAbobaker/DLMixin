{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 8.47936201095581\n",
      "Epoch 2/3, Loss: 1.5545154213905334\n",
      "Epoch 3/3, Loss: 0.39046511054039\n",
      "Fine-tuning with custom loss function:\n",
      "Epoch 1/3, Loss: 8.479362487792969\n",
      "Epoch 2/3, Loss: 1.5545153617858887\n",
      "Epoch 3/3, Loss: 0.39046511054039\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "class GPT2FineTuner(nn.Module):\n",
    "    def __init__(self, model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        # in future, replace with own pretrained gpt2 model\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs.loss, outputs.logits\n",
    "\n",
    "def prepare_data(instruction, response, tokenizer, max_length=512):\n",
    "    # Combine instruction and response\n",
    "    full_text = f\"Instruction: {instruction}\\nResponse: {response}\"\n",
    "    \n",
    "    # Tokenize and prepare input\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        full_text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids']\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    \n",
    "    # Prepare labels (shift input_ids right by one)\n",
    "    labels = input_ids.clone()\n",
    "    labels[:, :-1] = input_ids[:, 1:]\n",
    "    labels[:, -1] = -100  # Ignore loss for last token prediction\n",
    "    \n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "def train_step(model, optimizer, input_ids, attention_mask, labels):\n",
    "    optimizer.zero_grad()\n",
    "    loss, logits = model(input_ids, attention_mask, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), logits\n",
    "\n",
    "def custom_loss_vectorized(logits, labels):\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "    loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                           shift_labels.view(-1), \n",
    "                           ignore_index=-100,\n",
    "                           reduction='sum')\n",
    "    num_valid_tokens = (shift_labels != -100).sum().item()\n",
    "    return loss / num_valid_tokens if num_valid_tokens > 0 else 0\n",
    "\n",
    "def fine_tune(model, train_data, num_epochs=3, lr=5e-5):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for instruction, response in train_data:\n",
    "            input_ids, attention_mask, labels = prepare_data(instruction, response, model.tokenizer)\n",
    "            loss, _ = train_step(model, optimizer, input_ids, attention_mask, labels)\n",
    "            total_loss += loss\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_data)}\")\n",
    "    \n",
    "def fine_tune_custom_loss(model, train_data, num_epochs=3, lr=5e-5):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for instruction, response in train_data:\n",
    "            input_ids, attention_mask, labels = prepare_data(instruction, response, model.tokenizer)\n",
    "            _, logits = train_step(model, optimizer, input_ids, attention_mask, labels)\n",
    "            loss = custom_loss_vectorized(logits, labels)\n",
    "            total_loss += loss\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_data)}\")\n",
    "\n",
    "# Example usage\n",
    "model = GPT2FineTuner()\n",
    "train_data = [\n",
    "    (\"Explain the concept of machine learning.\", \"Machine learning is a subset of artificial intelligence...\"),\n",
    "    (\"What is a neural network?\", \"A neural network is a computational model inspired by the human brain...\")\n",
    "]\n",
    "fine_tune(model, train_data)\n",
    "model = GPT2FineTuner()\n",
    "print(\"Fine-tuning with custom loss function:\")\n",
    "fine_tune_custom_loss(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6310, 2762, 25, 1867, 318, 257, 17019, 3127, 30, 198, 31077, 25, 317, 17019, 3127, 318, 257, 31350, 2746, 7867]\n",
      "[2762, 25, 1867, 318, 257, 17019, 3127, 30, 198, 31077, 25, 317, 17019, 3127, 318, 257, 31350, 2746, 7867, 416]\n",
      "tensor([-100])\n"
     ]
    }
   ],
   "source": [
    "instruction = \"What is a neural network?\"\n",
    "response = \"A neural network is a computational model inspired by the human brain...\"\n",
    "full_text = f\"Instruction: {instruction}\\nResponse: {response}\"\n",
    "\n",
    "encoded = tokenizer.encode_plus(\n",
    "        full_text,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "\n",
    "input_ids = encoded['input_ids']\n",
    "attention_mask = encoded['attention_mask']\n",
    "\n",
    "# Prepare labels (shift input_ids right by one)\n",
    "labels = input_ids.clone()\n",
    "labels[:, :-1] = input_ids[:, 1:]\n",
    "labels[:, -1] = -100  # Ignore loss for last token prediction\n",
    "print(input_ids.view(-1).tolist()[:20])  \n",
    "print(labels.view(-1).tolist()[:20])\n",
    "print(labels[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Loss: 11.350384\n",
      "Vectorized Loss: 11.350384\n",
      "\n",
      "Verification:\n",
      "Loop Loss == Vectorized Loss: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# loss function without using huggingface library to calculate loss\n",
    "\n",
    "def custom_loss_loop(logits, labels):\n",
    "    loss = 0\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    num_valid_tokens = 0\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len - 1):  # -1 because we're predicting the next token\n",
    "            if labels[i, j + 1] != -100:\n",
    "                # predicted (number of tokens,)\n",
    "                predicted = logits[i, j, :]\n",
    "                # predicted (1,)\n",
    "                target = labels[i, j + 1]\n",
    "                # cross entropy expects 2D inputs\n",
    "                loss += F.cross_entropy(predicted.unsqueeze(0), target.unsqueeze(0), reduction='sum')\n",
    "                num_valid_tokens += 1\n",
    "    return loss / num_valid_tokens if num_valid_tokens > 0 else 0\n",
    "\n",
    "def custom_loss_vectorized(logits, labels):\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "    loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                           shift_labels.view(-1), \n",
    "                           ignore_index=-100,\n",
    "                           reduction='sum')\n",
    "    num_valid_tokens = (shift_labels != -100).sum().item()\n",
    "    return loss / num_valid_tokens if num_valid_tokens > 0 else 0\n",
    "\n",
    "\n",
    "# Example usage\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "batch_size, seq_len, vocab_size = 2, 10, 50000\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "labels[:, -1] = -100  # Set last token to -100\n",
    "\n",
    "loop_loss = custom_loss_loop(logits, labels)\n",
    "vectorized_loss = custom_loss_vectorized(logits, labels)\n",
    "\n",
    "print(f\"Loop Loss: {loop_loss.item():.6f}\")\n",
    "print(f\"Vectorized Loss: {vectorized_loss.item():.6f}\")\n",
    "\n",
    "# Verify that logits and labels are identical for all functions\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Loop Loss == Vectorized Loss: {torch.isclose(loop_loss, vectorized_loss)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000])\n",
      "torch.Size([1, 50000]) torch.Size([1])\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\D073999\\AppData\\Local\\miniconda3\\envs\\torchGPU\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # For reproducibility\n",
    "batch_size, seq_len, vocab_size = 2, 10, 50000\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "labels[:, -1] = -100  # Set last token to -100\n",
    "\n",
    "loss = 0\n",
    "batch_size, seq_len, vocab_size = logits.shape\n",
    "num_valid_tokens = 0\n",
    "for i in range(batch_size):\n",
    "    for j in range(seq_len - 1):  # -1 because we're predicting the next token\n",
    "        if labels[i, j + 1] != -100:\n",
    "            predicted = logits[i, j, :]\n",
    "            target = labels[i, j + 1]\n",
    "            #print(torch.topk(predicted, 5).indices, target)\n",
    "            print(predicted.shape)\n",
    "            print(predicted.unsqueeze(0).shape, target.unsqueeze(0).shape)\n",
    "            import sys; sys.exit()\n",
    " \n",
    "            loss += F.cross_entropy(predicted.unsqueeze(0), target.unsqueeze(0), reduction='sum')\n",
    "            num_valid_tokens += 1\n",
    "print(loss / num_valid_tokens if num_valid_tokens > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.3504)\n",
      "tensor(11.3504)\n"
     ]
    }
   ],
   "source": [
    "# Reduction: sum or mean does not matter in this case\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "batch_size, seq_len, vocab_size = 2, 10, 50000\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "labels[:, -1] = -100  # Set last token to -100\n",
    "\n",
    "loss = 0\n",
    "batch_size, seq_len, vocab_size = logits.shape\n",
    "num_valid_tokens = 0\n",
    "for i in range(batch_size):\n",
    "    for j in range(seq_len - 1):  # -1 because we're predicting the next token\n",
    "        if labels[i, j + 1] != -100:\n",
    "            predicted = logits[i, j, :]\n",
    "            target = labels[i, j + 1]\n",
    "            loss += F.cross_entropy(predicted.unsqueeze(0), target.unsqueeze(0), reduction='sum')\n",
    "            num_valid_tokens += 1\n",
    "print(loss / num_valid_tokens if num_valid_tokens > 0 else 0)\n",
    "\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "batch_size, seq_len, vocab_size = 2, 10, 50000\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "labels[:, -1] = -100  # Set last token to -100\n",
    "\n",
    "loss = 0\n",
    "batch_size, seq_len, vocab_size = logits.shape\n",
    "num_valid_tokens = 0\n",
    "for i in range(batch_size):\n",
    "    for j in range(seq_len - 1):  # -1 because we're predicting the next token\n",
    "        if labels[i, j + 1] != -100:\n",
    "            predicted = logits[i, j, :]\n",
    "            target = labels[i, j + 1]\n",
    "            loss += F.cross_entropy(predicted.unsqueeze(0), target.unsqueeze(0), reduction='mean')\n",
    "            num_valid_tokens += 1\n",
    "print(loss / num_valid_tokens if num_valid_tokens > 0 else 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
